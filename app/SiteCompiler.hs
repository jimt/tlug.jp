--------------------------------------------------------------------------------
{-# LANGUAGE OverloadedStrings #-}
import           Data.Monoid (mappend)
import           Hakyll
import           Hakyll.Web.Html (withUrls)
import           System.FilePath (joinPath, splitPath, replaceExtension)
import           Text.Pandoc (Pandoc, ReaderOptions, runPure, readMediaWiki)
import           Data.List (isPrefixOf)
import           Data.Text as DT (pack)

--------------------------------------------------------------------------------


main :: IO ()
main = hakyll $ do

    -- All our "just serve these files" content.
    -- Much may be stuff we should build from nicer source, but don't.
    match "docroot/**" $ do
        route   dropInitialComponent
        compile copyFileCompiler

    match "wiki/*" $ do
        route   idRoute     -- No extension; netlify config serves as text/html
        compile $ mediawikiCompiler
              >>= fixMediawikiUrls

    -- The rest of this is the sample code for a blog site from the
    -- initial project template. We're keeping this here as an example
    -- until we've extracted everything we need from it.

    match "example/images/*" $ do
        route   idRoute
        compile copyFileCompiler

    match "example/css/*" $ do
        route   idRoute
        compile compressCssCompiler

    match (fromList ["example/about.rst", "example/contact.markdown"]) $ do
        route   $ setExtension "html"
        compile $ pandocCompiler
            >>= loadAndApplyTemplate
                "example/templates/default.html" defaultContext
            >>= relativizeUrls

    match "example/posts/*" $ do
        route $ setExtension "html"
        compile $ pandocCompiler
            >>= loadAndApplyTemplate "example/templates/post.html"    postCtx
            >>= loadAndApplyTemplate "example/templates/default.html" postCtx
            >>= relativizeUrls

    create ["example/archive.html"] $ do
        route idRoute
        compile $ do
            posts <- recentFirst =<< loadAll "example/posts/*"
            let archiveCtx =
                    listField "posts" postCtx (return posts) `mappend`
                    constField "title" "Archives"            `mappend`
                    defaultContext
            makeItem ""
                >>= loadAndApplyTemplate
                    "example/templates/archive.html" archiveCtx
                >>= loadAndApplyTemplate
                    "example/templates/default.html" archiveCtx
                >>= relativizeUrls

    match "example/index.html" $ do
        route idRoute
        compile $ do
            posts <- recentFirst =<< loadAll "example/posts/*"
            let indexCtx =
                    listField "posts" postCtx (return posts) `mappend`
                    constField "title" "Home"                `mappend`
                    defaultContext
            getResourceBody
                >>= applyAsTemplate indexCtx
                >>= loadAndApplyTemplate
                    "example/templates/default.html" indexCtx
                >>= relativizeUrls

    match "example/templates/*" $ compile templateCompiler


--------------------------------------------------------------------------------
-- Custom code for our site

-- | Drop the given number of leading path components
dropInitialComponents :: Int -> Routes
dropInitialComponents n = customRoute $
    joinPath . drop n . splitPath . toFilePath

dropInitialComponent = dropInitialComponents 1

mediawikiCompiler :: Compiler (Item String)
mediawikiCompiler =
     do markup <- getResourceBody
        pandoc <- read defaultHakyllReaderOptions markup
        return $ writePandoc pandoc
    where
        ropt = defaultHakyllReaderOptions
        -- This is a copy of Hakyll.Web.Pandoc.readPandocWith the first
        -- argument to `traverse` replaced with `readMediaWiki ropt`
        -- because the original function is hardcoded to select the Pandoc
        -- read function based on the source file extension, which our
        -- source files do not have.
        read :: ReaderOptions -> Item String -> Compiler (Item Pandoc)
        read ropt item =
            case runPure $ traverse (readMediaWiki ropt) (fmap DT.pack item) of
                 Left err    -> fail $ "MediaWiki parse failed: " ++ show err
                 Right item' -> return item'


-- | Fix namespaced links generated by the mediawikiCompiler.
--
-- The mediawikiCompiler/pandoc produces links to other pages within the
-- site using relative URLs containing just the page name. This works fine
-- when the page has no namespace (@href="TLUG_Timeline"@) but breaks in
-- browsers when a namespace is present, as in @href="TLUG:Organization"@,
-- because that's interpreted as @tlug://Organization@ rather than
-- @http://www.tlug.jp/wiki/TLUG:Organization@.
--
-- MediaWiki deals with these by generating @/wiki/...@ URLs for all pages
-- within the wiki. Unfortunately, we can't tell at this stage whether
-- Pandoc was rendering an internal or external link, so we use a heuristic
-- of "see if the URL prefix is in the list of all namespaces we know
-- about" and add a @/wiki/@ in front of the URL if it is. If we find links
-- that this does not fix, we can update the list of known prefixes below.
--
fixMediawikiUrls :: Item String -> Compiler (Item String)
fixMediawikiUrls item = do
    return $ fmap (withUrls fixLink) item
    where
        namespaces =
            [ ":", "Special:", "User:"              -- MediaWiki standard
            , "TLUG:", "TlugWiki:", "TlugAdmin:"    -- Custom to TLUG
            ]
        fixLink url | isNamespaced url namespaces = "/wiki/" ++ url
                    | otherwise                   =             url
        isNamespaced s [] = False
        isNamespaced s (p:ps) | p `isPrefixOf` s = True
                              | otherwise        = isNamespaced s ps

--------------------------------------------------------------------------------
-- Also sample blog site code

postCtx :: Context String
postCtx =
    dateField "date" "%B %e, %Y" `mappend`
    defaultContext
